# Project Zero (ZeroLang)

**Human-readable code is a legacy format. It's time for AI-native execution.**

## The Vision
ZeroLang is an experimental project to eliminate the "Human Syntax Tax".
Instead of AI writing Python/JS that needs to be parsed and compiled, we are training AI to output optimized **WebAssembly (WASM)** directly.

**Equation:** `Prompt â†’ LLM â†’ Optimized Binary (.zero) â†’ Execution`

## Why?
1. **Efficiency:** Skip the parsing/lexing/compiling overhead.
2. **Context:** Token-optimized format means 3x more logic in the same context window.
3. **Portability:** Runs on Edge, Cloud, and Browser via WASM.

## Current Status âœ…

| Phase | Status | Description |
|-------|--------|-------------|
| Data Collection | âœ… Complete | 1000+ Câ†’WAT pairs from 48 repos |
| Model Training | âœ… Complete | Qwen2.5-Coder-14B fine-tuned on H100 |
| API Deployment | âœ… Live | Gradio API on Colab |
| CLI Runtime | âœ… Working | End-to-end execution via `zerolang_cli.py` |

## Quick Start

### 1. Install Dependencies
```bash
# Clone the repo
git clone https://github.com/user/project-zero
cd project-zero

# Create venv and install
python3 -m venv .venv
source .venv/bin/activate
pip install wasmtime gradio_client

# Install wasm-tools (for WATâ†’WASM conversion)
# macOS:
brew install wasm-tools

# Or download from: https://github.com/bytecodealliance/wasm-tools/releases
```

### 2. Run the CLI
```bash
# Start interactive CLI (replace with your Gradio URL)
python zerolang_cli.py --api https://YOUR-GRADIO-URL.gradio.live
```

### 3. Generate and Execute
```
zerolang> gen Implement: int add(int a, int b)
[âœ“] Generated 30 lines of WAT

zerolang> run add(5, 3)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Result:                    8  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

zerolang> genrun Implement: int max(int a, int b) | max(10, 25)
[âœ“] Result: 25
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ZeroLang Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ Natural  â”‚â”€â”€â”€â–¶â”‚ Fine-tuned  â”‚â”€â”€â”€â–¶â”‚   WAT    â”‚â”€â”€â”€â–¶â”‚ WASM  â”‚ â”‚
â”‚   â”‚ Language â”‚    â”‚   LLM       â”‚    â”‚  Code    â”‚    â”‚ Binaryâ”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚      â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚
â”‚                                          â”‚   wasmtime        â”‚ â”‚
â”‚                                          â”‚   (Execution)     â”‚ â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure
```
project-zero/
â”œâ”€â”€ zerolang_cli.py       # ğŸ¯ Main CLI - Start here!
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ generator.py      # C â†’ WAT data pipeline
â”‚   â”œâ”€â”€ collect_large.py  # Large-scale data collection
â”‚   â””â”€â”€ postprocess.py    # Merge, dedup, split
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_cloud.py    # Cloud training script
â”‚   â””â”€â”€ inference.py      # Local inference test
â”œâ”€â”€ zrun/
â”‚   â””â”€â”€ runtime.py        # WASM execution runtime
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Step1_Collect_Data.ipynb  # Colab data collection
â”‚   â”œâ”€â”€ Step2_Train_Model.ipynb   # Colab training (H100)
â”‚   â””â”€â”€ Test_Environment.ipynb    # Environment verification
â””â”€â”€ data/                 # Generated training data
```

## CLI Commands

| Command | Description | Example |
|---------|-------------|---------|
| `gen <instruction>` | Generate WAT from instruction | `gen Implement: int add(int a, int b)` |
| `run <call>` | Execute last generated WAT | `run add(5, 3)` |
| `genrun <instr> \| <call>` | Generate and run in one step | `genrun Implement: int mul(int a, int b) \| mul(6, 7)` |
| `wat` | Show last generated WAT | |
| `clear` | Clear screen | |
| `help` | Show help | |
| `quit` | Exit | |

## Training Your Own Model

See [CLOUD_TRAINING.md](CLOUD_TRAINING.md) for detailed instructions on:
1. Collecting training data (free CPU on Colab)
2. Training the model (H100 GPU on Colab)
3. Deploying the API

## Tech Stack

| Component | Technology |
|-----------|------------|
| Training Data | C code from GitHub â†’ WAT via LLVM |
| Model | Qwen2.5-Coder-14B (LoRA fine-tuned) |
| API | Gradio (hosted on Colab) |
| Runtime | wasmtime (Python + wasm-tools) |
| CLI | Python (zerolang_cli.py) |

## Limitations

- Currently works best with simple mathematical functions
- Recursive functions may hit stack limits
- No I/O operations (console, file) - pure computation only

## Roadmap

- [x] Phase 1: Data Collection Pipeline
- [x] Phase 2: Model Fine-Tuning
- [x] Phase 3: CLI Runtime
- [ ] Phase 4: Larger dataset (10k+ examples)
- [ ] Phase 5: Support for more complex programs
- [ ] Phase 6: Local model deployment (quantized)

---

*"The best code is no code. The second best is code humans never have to read."*
