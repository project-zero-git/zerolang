{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Step 2: Train Model (H100 GPU)\n",
        "\n",
        "**REQUIRES GPU** - Run this with H100 runtime!\n",
        "\n",
        "This notebook:\n",
        "1. Loads data from Google Drive (from Step 1)\n",
        "2. Trains Qwen2.5-Coder-14B with LoRA\n",
        "3. Exports trained model\n",
        "\n",
        "**Time:** ~1 hour\n",
        "**Cost:** ~$4 (H100)\n",
        "\n",
        "**Prerequisites:** Run **Step1_Collect_Data.ipynb** first!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "\n",
        "MODEL = \"qwen-coder-14b\"  #@param [\"qwen-coder-7b\", \"qwen-coder-14b\", \"qwen-coder-32b\"]\n",
        "EPOCHS = 5  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
        "MAX_LENGTH = 2048  #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 2e-4  #@param {type:\"number\"}\n",
        "\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(f\"Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, Max Length: {MAX_LENGTH}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No GPU found! Change runtime to H100.\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"\\n‚úì GPU: {gpu_name} ({gpu_mem:.0f} GB)\")\n",
        "\n",
        "if gpu_mem < 40:\n",
        "    print(\"‚ö†Ô∏è Warning: Less than 40GB VRAM. Consider using qwen-coder-7b.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_DATA = '/content/drive/MyDrive/zerolang_data'\n",
        "DRIVE_OUTPUT = '/content/drive/MyDrive/zerolang_models'\n",
        "\n",
        "!mkdir -p {DRIVE_OUTPUT}\n",
        "\n",
        "# Check data exists\n",
        "import os\n",
        "required_files = ['train_chatml_large.jsonl', 'val_chatml_large.jsonl']\n",
        "missing = [f for f in required_files if not os.path.exists(f\"{DRIVE_DATA}/{f}\")]\n",
        "\n",
        "if missing:\n",
        "    print(f\"‚ùå Missing files in Google Drive: {missing}\")\n",
        "    print(f\"\\nPlease run Step1_Collect_Data.ipynb first!\")\n",
        "    raise FileNotFoundError(\"Training data not found\")\n",
        "else:\n",
        "    print(f\"‚úì Training data found in {DRIVE_DATA}\")\n",
        "    !ls -lh {DRIVE_DATA}/*.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repo and setup\n",
        "!rm -rf /content/zerolang\n",
        "!git clone --depth=1 https://github.com/project-zero-git/zerolang.git /content/zerolang\n",
        "%cd /content/zerolang\n",
        "\n",
        "# Copy data from Drive to local (faster training)\n",
        "!mkdir -p data\n",
        "!cp {DRIVE_DATA}/train_chatml_large.jsonl data/\n",
        "!cp {DRIVE_DATA}/val_chatml_large.jsonl data/\n",
        "\n",
        "# Count samples\n",
        "!echo \"Training samples:\" && wc -l data/train_chatml_large.jsonl\n",
        "!echo \"Validation samples:\" && wc -l data/val_chatml_large.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update train_cloud.py to find our data files\n",
        "# Add large dataset file names to the search list\n",
        "\n",
        "train_script = open('training/train_cloud.py').read()\n",
        "if 'train_chatml_large.jsonl' not in train_script:\n",
        "    train_script = train_script.replace(\n",
        "        '\"train_chatml_colab.jsonl\"',\n",
        "        '\"train_chatml_large.jsonl\",\\n        \"train_chatml_colab.jsonl\"'\n",
        "    )\n",
        "    train_script = train_script.replace(\n",
        "        '\"val_chatml_colab.jsonl\"',\n",
        "        '\"val_chatml_large.jsonl\",\\n        \"val_chatml_colab.jsonl\"'\n",
        "    )\n",
        "    with open('training/train_cloud.py', 'w') as f:\n",
        "        f.write(train_script)\n",
        "    print(\"‚úì Updated train_cloud.py to find large dataset\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train!\n",
        "output_dir = f\"models/zerolang-{MODEL}-large\"\n",
        "\n",
        "print(f\"Training {MODEL}...\")\n",
        "print(f\"Output: {output_dir}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "!python training/train_cloud.py \\\n",
        "    --model {MODEL} \\\n",
        "    --data data \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --batch-size {BATCH_SIZE} \\\n",
        "    --max-length {MAX_LENGTH} \\\n",
        "    --lr {LEARNING_RATE} \\\n",
        "    --output {output_dir}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "BASE_MODELS = {\n",
        "    \"qwen-coder-7b\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    \"qwen-coder-14b\": \"Qwen/Qwen2.5-Coder-14B-Instruct\",\n",
        "    \"qwen-coder-32b\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "}\n",
        "\n",
        "model_path = f\"models/zerolang-{MODEL}-large\"\n",
        "base_model_name = BASE_MODELS[MODEL]\n",
        "\n",
        "print(f\"Loading model from {model_path}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "model.eval()\n",
        "print(\"‚úì Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are ZeroLang, an AI that generates WebAssembly (WAT) code.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=512, temperature=0.2, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
        "    \n",
        "    return tokenizer.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# Test prompts\n",
        "tests = [\n",
        "    \"Implement: int add(int a, int b)\",\n",
        "    \"Implement: int factorial(int n)\",\n",
        "    \"Implement: int max(int a, int b)\",\n",
        "]\n",
        "\n",
        "for prompt in tests:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\">>> {prompt}\")\n",
        "    print('='*50)\n",
        "    result = generate(prompt)\n",
        "    print(result[:500] if len(result) > 500 else result)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Google Drive\n",
        "import shutil\n",
        "\n",
        "output_name = f\"zerolang-{MODEL}-large\"\n",
        "drive_model_path = f\"{DRIVE_OUTPUT}/{output_name}\"\n",
        "\n",
        "print(f\"Saving model to Google Drive...\")\n",
        "if os.path.exists(drive_model_path):\n",
        "    shutil.rmtree(drive_model_path)\n",
        "shutil.copytree(f\"models/{output_name}\", drive_model_path)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ Model saved to: {drive_model_path}\")\n",
        "print(\"=\"*60)\n",
        "!ls -la {drive_model_path}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create downloadable zip\n",
        "!cd models && zip -r ../zerolang-model-large.zip {output_name}\n",
        "print(f\"\\nZip file created: zerolang-model-large.zip\")\n",
        "!ls -lh zerolang-model-large.zip\n",
        "\n",
        "# Uncomment to download:\n",
        "# from google.colab import files\n",
        "# files.download('zerolang-model-large.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: {MODEL}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"\\nSaved to:\")\n",
        "print(f\"  - Google Drive: {drive_model_path}\")\n",
        "print(f\"  - Zip: zerolang-model-large.zip\")\n",
        "print(f\"\\nTo use locally:\")\n",
        "print(f\"  python scripts/download_and_test.py --model-path <path-to-model>\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
