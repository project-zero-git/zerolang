{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ ZeroLang Full Pipeline\n",
        "\n",
        "**Automatic data collection + training in one notebook**\n",
        "\n",
        "1. Collect 2000+ C‚ÜíWAT training pairs\n",
        "2. Train Qwen2.5-Coder-14B model\n",
        "3. Test and export\n",
        "\n",
        "**Total time: ~3-4 hours**\n",
        "- Data collection: ~2 hours\n",
        "- Training: ~1-2 hours\n",
        "\n",
        "**Requirements:**\n",
        "- H100 GPU (or A100 for 7B model)\n",
        "- Colab Pro+ recommended"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### Data Collection\n",
        "TARGET_PAIRS = 2000  #@param {type:\"integer\"}\n",
        "MAX_REPOS = 50  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Training\n",
        "MODEL = \"qwen-coder-14b\"  #@param [\"qwen-coder-7b\", \"qwen-coder-14b\", \"qwen-coder-32b\"]\n",
        "EPOCHS = 10  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
        "MAX_LENGTH = 2048  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Output\n",
        "SAVE_TO_DRIVE = True  #@param {type:\"boolean\"}\n",
        "\n",
        "print(f\"Target: {TARGET_PAIRS} pairs from {MAX_REPOS} repos\")\n",
        "print(f\"Model: {MODEL}, Epochs: {EPOCHS}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "!apt-get install -qq clang lld  # For C compilation\n",
        "\n",
        "# Install wasm-tools\n",
        "!curl -LO https://github.com/aspect-build/wasm-tools/releases/download/v1.230.0/wasm-tools-linux-x86_64.tar.gz\n",
        "!tar -xzf wasm-tools-linux-x86_64.tar.gz\n",
        "!mv wasm-tools /usr/local/bin/\n",
        "!wasm-tools --version"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/project-zero-git/zerolang.git\n",
        "%cd zerolang"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (optional - for saving model)\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_OUTPUT = '/content/drive/MyDrive/zerolang_models'\n",
        "    !mkdir -p {DRIVE_OUTPUT}\n",
        "    print(f\"Models will be saved to: {DRIVE_OUTPUT}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Data Collection\n",
        "\n",
        "Collects C functions from GitHub repos and compiles to WAT"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended repository list for more data\n",
        "REPOS = '''\n",
        "# Algorithms & Data Structures\n",
        "https://github.com/TheAlgorithms/C\n",
        "https://github.com/fragglet/c-algorithms\n",
        "https://github.com/attractivechaos/klib\n",
        "https://github.com/srdja/Collections-C\n",
        "https://github.com/troydhanson/uthash\n",
        "\n",
        "# Cryptography\n",
        "https://github.com/B-Con/crypto-algorithms\n",
        "https://github.com/kokke/tiny-AES-c\n",
        "https://github.com/ctz/cifra\n",
        "https://github.com/983/SHA-256\n",
        "https://github.com/983/Num\n",
        "\n",
        "# String & Text\n",
        "https://github.com/sheredom/utf8.h\n",
        "https://github.com/antirez/sds\n",
        "https://github.com/jwerle/murmurhash.c\n",
        "https://github.com/skeeto/branchless-utf8\n",
        "\n",
        "# JSON & Parsing\n",
        "https://github.com/DaveGamble/cJSON\n",
        "https://github.com/zserge/jsmn\n",
        "https://github.com/kgabis/parson\n",
        "https://github.com/cesanta/frozen\n",
        "\n",
        "# Compression\n",
        "https://github.com/lz4/lz4\n",
        "https://github.com/richgel999/miniz\n",
        "https://github.com/ebiggers/libdeflate\n",
        "\n",
        "# Math & Numerical\n",
        "https://github.com/nothings/stb\n",
        "https://github.com/983/fft\n",
        "https://github.com/skeeto/hash-prospector\n",
        "https://github.com/lemire/clhash\n",
        "\n",
        "# Utilities\n",
        "https://github.com/antirez/linenoise\n",
        "https://github.com/rxi/vec\n",
        "https://github.com/rxi/map\n",
        "https://github.com/rxi/log.c\n",
        "https://github.com/skeeto/optparse\n",
        "https://github.com/gingerBill/gb\n",
        "https://github.com/mackron/dr_libs\n",
        "\n",
        "# Embedded\n",
        "https://github.com/cesanta/mongoose\n",
        "https://github.com/nodejs/http-parser\n",
        "\n",
        "# Additional algorithm repos\n",
        "https://github.com/tezc/sc\n",
        "https://github.com/tidwall/hashmap.c\n",
        "https://github.com/sheredom/hashmap.h\n",
        "https://github.com/tidwall/btree.c\n",
        "https://github.com/antirez/rax\n",
        "https://github.com/orangeduck/mpc\n",
        "https://github.com/pervognsen/bitwise\n",
        "https://github.com/clibs/buffer\n",
        "https://github.com/clibs/list\n",
        "https://github.com/michaelrsweet/mxml\n",
        "'''\n",
        "\n",
        "# Save to file\n",
        "with open('pipeline/repos_extended.txt', 'w') as f:\n",
        "    f.write(REPOS)\n",
        "\n",
        "repo_count = len([l for l in REPOS.strip().split('\\n') if l.strip() and not l.startswith('#')])\n",
        "print(f\"Total repos: {repo_count}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update generator.py clang path for Colab\n",
        "!sed -i 's|/opt/homebrew/opt/llvm/bin/clang|clang|g' pipeline/generator.py\n",
        "\n",
        "# Verify clang works with WASM target\n",
        "!echo 'int add(int a, int b) { return a + b; }' > /tmp/test.c\n",
        "!clang --target=wasm32 -c /tmp/test.c -o /tmp/test.o 2>&1 || echo \"Clang WASM check done\""
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run data collection\n",
        "print(f\"Collecting data from repos (target: {TARGET_PAIRS} pairs)...\")\n",
        "print(\"This will take ~1-2 hours...\\n\")\n",
        "\n",
        "!python pipeline/generator.py \\\n",
        "    -l pipeline/repos_extended.txt \\\n",
        "    -o data/colab_training.jsonl \\\n",
        "    2>&1 | tee data/collection.log | grep -E '(SUCCESS|Processing|pairs_generated)'"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check collected data\n",
        "!wc -l data/colab_training.jsonl\n",
        "\n",
        "import json\n",
        "with open('data/colab_training.jsonl') as f:\n",
        "    pairs = [json.loads(l) for l in f if l.strip()]\n",
        "\n",
        "print(f\"\\nCollected {len(pairs)} training pairs\")\n",
        "print(f\"Avg instruction length: {sum(len(p['instruction']) for p in pairs)/len(pairs):.0f} chars\")\n",
        "print(f\"Avg WAT length: {sum(len(p['output']) for p in pairs)/len(pairs):.0f} chars\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/val and convert to ChatML\n",
        "!python pipeline/postprocess.py split data/colab_training.jsonl \\\n",
        "    --train data/train_colab.jsonl \\\n",
        "    --val data/val_colab.jsonl \\\n",
        "    --val-ratio 0.1\n",
        "\n",
        "!python training/prepare_data.py data/train_colab.jsonl -o data/train_chatml_colab.jsonl -f chatml\n",
        "!python training/prepare_data.py data/val_colab.jsonl -o data/val_chatml_colab.jsonl -f chatml\n",
        "\n",
        "!wc -l data/*_colab.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Model Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train model\n",
        "print(f\"Training {MODEL} for {EPOCHS} epochs...\")\n",
        "print(\"This will take ~1-2 hours...\\n\")\n",
        "\n",
        "!python training/train_cloud.py \\\n",
        "    --model {MODEL} \\\n",
        "    --data data \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --batch-size {BATCH_SIZE} \\\n",
        "    --max-length {MAX_LENGTH} \\\n",
        "    --output models/zerolang-{MODEL}-colab"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Test Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Model mapping\n",
        "BASE_MODELS = {\n",
        "    \"qwen-coder-7b\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    \"qwen-coder-14b\": \"Qwen/Qwen2.5-Coder-14B-Instruct\",\n",
        "    \"qwen-coder-32b\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "}\n",
        "\n",
        "model_path = f\"models/zerolang-{MODEL}-colab\"\n",
        "base_model_name = BASE_MODELS[MODEL]\n",
        "\n",
        "print(f\"Loading {model_path}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, model_path)\n",
        "model.eval()\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_wat(instruction, max_tokens=1024):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are ZeroLang, an AI that generates optimized WebAssembly (WAT) code. Output only valid WAT code.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction},\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract assistant response\n",
        "    if \"assistant\" in response.lower():\n",
        "        response = response.split(\"assistant\")[-1].strip()\n",
        "    return response"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with various prompts\n",
        "test_prompts = [\n",
        "    \"Implement: int add(int a, int b)\",\n",
        "    \"Implement: int factorial(int n)\",\n",
        "    \"Implement: void swap(int *a, int *b)\",\n",
        "    \"Implement: int max(int a, int b)\",\n",
        "    \"Implement: int fibonacci(int n)\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Input: {prompt}\")\n",
        "    print('='*60)\n",
        "    wat = generate_wat(prompt)\n",
        "    print(wat[:800] if len(wat) > 800 else wat)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Save & Export"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Google Drive\n",
        "if SAVE_TO_DRIVE:\n",
        "    import shutil\n",
        "    output_name = f\"zerolang-{MODEL}-colab\"\n",
        "    \n",
        "    # Copy model\n",
        "    shutil.copytree(f\"models/{output_name}\", f\"{DRIVE_OUTPUT}/{output_name}\", dirs_exist_ok=True)\n",
        "    \n",
        "    # Copy data\n",
        "    shutil.copy(\"data/colab_training.jsonl\", f\"{DRIVE_OUTPUT}/training_data.jsonl\")\n",
        "    \n",
        "    print(f\"‚úÖ Saved to Google Drive: {DRIVE_OUTPUT}\")\n",
        "    !ls -la {DRIVE_OUTPUT}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or download as zip\n",
        "!zip -r zerolang-model.zip models/zerolang-{MODEL}-colab data/colab_training.jsonl\n",
        "\n",
        "from google.colab import files\n",
        "files.download('zerolang-model.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Summary"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üéâ ZeroLang Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nData collected: {len(pairs)} pairs\")\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"\\nOutput: models/zerolang-{MODEL}-colab\")\n",
        "if SAVE_TO_DRIVE:\n",
        "    print(f\"Google Drive: {DRIVE_OUTPUT}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
