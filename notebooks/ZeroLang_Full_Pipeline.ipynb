{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ ZeroLang Full Pipeline\n",
        "\n",
        "**Automatic data collection + training in one notebook**\n",
        "\n",
        "1. Collect 2000+ C‚ÜíWAT training pairs\n",
        "2. Train Qwen2.5-Coder-14B model\n",
        "3. Test and export\n",
        "\n",
        "**Total time: ~3-4 hours**\n",
        "- Data collection: ~2 hours\n",
        "- Training: ~1-2 hours\n",
        "\n",
        "**Requirements:**\n",
        "- H100 GPU (or A100 for 7B model)\n",
        "- Colab Pro+ recommended"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### Data Collection\n",
        "TARGET_PAIRS = 2000  #@param {type:\"integer\"}\n",
        "MAX_REPOS = 50  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Training\n",
        "MODEL = \"qwen-coder-14b\"  #@param [\"qwen-coder-7b\", \"qwen-coder-14b\", \"qwen-coder-32b\"]\n",
        "EPOCHS = 10  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 8  #@param {type:\"integer\"}\n",
        "MAX_LENGTH = 2048  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Output\n",
        "SAVE_TO_DRIVE = True  #@param {type:\"boolean\"}\n",
        "\n",
        "print(f\"Target: {TARGET_PAIRS} pairs from {MAX_REPOS} repos\")\n",
        "print(f\"Model: {MODEL}, Epochs: {EPOCHS}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Install LLVM with WASM support\n",
        "echo \"[1/4] Installing LLVM toolchain...\"\n",
        "apt-get update -qq\n",
        "apt-get install -qq -y llvm lld clang > /dev/null 2>&1\n",
        "\n",
        "# Verify WASM target works\n",
        "echo \"[2/4] Testing WASM compilation...\"\n",
        "echo 'int add(int a, int b) { return a + b; }' > /tmp/test.c\n",
        "clang --target=wasm32 -nostdlib -Wl,--no-entry -Wl,--export-all -fuse-ld=lld -o /tmp/test.wasm /tmp/test.c 2>/dev/null\n",
        "if [ -f /tmp/test.wasm ]; then\n",
        "    echo \"‚úì WASM compilation works!\"\n",
        "else\n",
        "    echo \"‚úó WASM compilation failed - trying alternative...\"\n",
        "    # Try with explicit lld path\n",
        "    clang --target=wasm32 -nostdlib -Wl,--no-entry -Wl,--export-all -fuse-ld=/usr/bin/lld -o /tmp/test.wasm /tmp/test.c\n",
        "fi\n",
        "\n",
        "# Install wasm-tools from bytecodealliance\n",
        "echo \"[3/4] Installing wasm-tools...\"\n",
        "cd /tmp\n",
        "curl -sLO https://github.com/bytecodealliance/wasm-tools/releases/download/v1.244.0/wasm-tools-1.244.0-x86_64-linux.tar.gz\n",
        "tar -xzf wasm-tools-1.244.0-x86_64-linux.tar.gz\n",
        "cp wasm-tools-1.244.0-x86_64-linux/wasm-tools /usr/local/bin/\n",
        "chmod +x /usr/local/bin/wasm-tools\n",
        "\n",
        "# Verify wasm-tools\n",
        "echo \"[4/4] Testing wasm-tools...\"\n",
        "wasm-tools print /tmp/test.wasm > /dev/null 2>&1\n",
        "if [ $? -eq 0 ]; then\n",
        "    echo \"‚úì wasm-tools works!\"\n",
        "    wasm-tools --version\n",
        "else\n",
        "    echo \"‚úó wasm-tools failed\"\n",
        "fi\n",
        "\n",
        "echo \"\"\n",
        "echo \"=== Environment Ready ===\""
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python dependencies\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repository\n",
        "!rm -rf zerolang  # Clean if exists\n",
        "!git clone --depth=1 https://github.com/project-zero-git/zerolang.git\n",
        "%cd zerolang\n",
        "!ls -la"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update generator.py to use system clang (not homebrew path)\n",
        "!sed -i 's|/opt/homebrew/opt/llvm/bin/clang|clang|g' pipeline/generator.py\n",
        "\n",
        "# Quick test: compile a single function\n",
        "print(\"Testing pipeline with a single function...\")\n",
        "!echo 'int multiply(int a, int b) { return a * b; }' > /tmp/single_test.c\n",
        "!clang --target=wasm32 -O2 -nostdlib -fuse-ld=lld -Wl,--no-entry -Wl,--export-all -o /tmp/single_test.wasm /tmp/single_test.c && wasm-tools print /tmp/single_test.wasm | head -20\n",
        "print(\"\\n‚úì Pipeline test passed!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (optional - for saving model)\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_OUTPUT = '/content/drive/MyDrive/zerolang_models'\n",
        "    !mkdir -p {DRIVE_OUTPUT}\n",
        "    print(f\"Models will be saved to: {DRIVE_OUTPUT}\")\n",
        "else:\n",
        "    DRIVE_OUTPUT = None\n",
        "    print(\"Drive not mounted - model will be saved locally only\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Data Collection\n",
        "\n",
        "Collects C functions from GitHub repos and compiles to WAT"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended repository list for more data\n",
        "REPOS = '''# Algorithms & Data Structures\n",
        "https://github.com/TheAlgorithms/C\n",
        "https://github.com/fragglet/c-algorithms\n",
        "https://github.com/attractivechaos/klib\n",
        "https://github.com/srdja/Collections-C\n",
        "https://github.com/troydhanson/uthash\n",
        "\n",
        "# Cryptography\n",
        "https://github.com/B-Con/crypto-algorithms\n",
        "https://github.com/kokke/tiny-AES-c\n",
        "https://github.com/ctz/cifra\n",
        "https://github.com/983/SHA-256\n",
        "https://github.com/983/Num\n",
        "\n",
        "# String & Text\n",
        "https://github.com/sheredom/utf8.h\n",
        "https://github.com/antirez/sds\n",
        "https://github.com/jwerle/murmurhash.c\n",
        "https://github.com/skeeto/branchless-utf8\n",
        "\n",
        "# JSON & Parsing\n",
        "https://github.com/DaveGamble/cJSON\n",
        "https://github.com/zserge/jsmn\n",
        "https://github.com/kgabis/parson\n",
        "https://github.com/cesanta/frozen\n",
        "\n",
        "# Compression\n",
        "https://github.com/lz4/lz4\n",
        "https://github.com/richgel999/miniz\n",
        "\n",
        "# Math & Numerical\n",
        "https://github.com/nothings/stb\n",
        "https://github.com/983/fft\n",
        "https://github.com/skeeto/hash-prospector\n",
        "https://github.com/lemire/clhash\n",
        "\n",
        "# Utilities\n",
        "https://github.com/antirez/linenoise\n",
        "https://github.com/rxi/vec\n",
        "https://github.com/rxi/map\n",
        "https://github.com/rxi/log.c\n",
        "https://github.com/skeeto/optparse\n",
        "https://github.com/gingerBill/gb\n",
        "\n",
        "# Additional algorithm repos\n",
        "https://github.com/tezc/sc\n",
        "https://github.com/tidwall/hashmap.c\n",
        "https://github.com/sheredom/hashmap.h\n",
        "https://github.com/tidwall/btree.c\n",
        "https://github.com/antirez/rax\n",
        "https://github.com/clibs/buffer\n",
        "https://github.com/clibs/list\n",
        "'''\n",
        "\n",
        "# Save to file\n",
        "with open('pipeline/repos_extended.txt', 'w') as f:\n",
        "    f.write(REPOS)\n",
        "\n",
        "repo_list = [l.strip() for l in REPOS.strip().split('\\n') if l.strip() and not l.startswith('#')]\n",
        "print(f\"Total repos to process: {len(repo_list)}\")\n",
        "for i, r in enumerate(repo_list, 1):\n",
        "    print(f\"  {i}. {r.split('/')[-1]}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run data collection\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "print(f\"Collecting data from repos...\")\n",
        "print(\"This will take ~1-2 hours...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!python pipeline/generator.py \\\n",
        "    -l pipeline/repos_extended.txt \\\n",
        "    -o data/colab_training.jsonl \\\n",
        "    --verbose \\\n",
        "    2>&1 | tee data/collection.log | grep -E '(SUCCESS|Processing:|pairs_generated|==)'"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check collected data\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_file = Path('data/colab_training.jsonl')\n",
        "if not data_file.exists():\n",
        "    print(\"ERROR: No data collected!\")\n",
        "    print(\"Check data/collection.log for errors\")\n",
        "else:\n",
        "    with open(data_file) as f:\n",
        "        pairs = [json.loads(l) for l in f if l.strip()]\n",
        "    \n",
        "    print(f\"=\"*60)\n",
        "    print(f\"Data Collection Results\")\n",
        "    print(f\"=\"*60)\n",
        "    print(f\"Total pairs collected: {len(pairs)}\")\n",
        "    \n",
        "    if pairs:\n",
        "        avg_instr = sum(len(p['instruction']) for p in pairs) / len(pairs)\n",
        "        avg_wat = sum(len(p['output']) for p in pairs) / len(pairs)\n",
        "        print(f\"Avg instruction length: {avg_instr:.0f} chars\")\n",
        "        print(f\"Avg WAT length: {avg_wat:.0f} chars\")\n",
        "        \n",
        "        # Sample\n",
        "        print(f\"\\n--- Sample pair ---\")\n",
        "        print(f\"Instruction: {pairs[0]['instruction'][:100]}...\")\n",
        "        print(f\"WAT preview: {pairs[0]['output'][:200]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/val and convert to ChatML format\n",
        "if len(pairs) > 0:\n",
        "    print(\"Splitting data...\")\n",
        "    !python pipeline/postprocess.py split data/colab_training.jsonl \\\n",
        "        --train data/train_colab.jsonl \\\n",
        "        --val data/val_colab.jsonl \\\n",
        "        --val-ratio 0.1\n",
        "    \n",
        "    print(\"\\nConverting to ChatML format...\")\n",
        "    !python training/prepare_data.py data/train_colab.jsonl -o data/train_chatml_colab.jsonl -f chatml\n",
        "    !python training/prepare_data.py data/val_colab.jsonl -o data/val_chatml_colab.jsonl -f chatml\n",
        "    \n",
        "    print(\"\\nFinal data files:\")\n",
        "    !wc -l data/*_colab.jsonl\n",
        "else:\n",
        "    print(\"ERROR: No data to process. Check collection logs.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Model Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data is ready before training\n",
        "train_file = Path('data/train_chatml_colab.jsonl')\n",
        "val_file = Path('data/val_chatml_colab.jsonl')\n",
        "\n",
        "if not train_file.exists() or not val_file.exists():\n",
        "    raise FileNotFoundError(\"Training data not found! Run data collection first.\")\n",
        "\n",
        "with open(train_file) as f:\n",
        "    train_count = sum(1 for _ in f)\n",
        "with open(val_file) as f:\n",
        "    val_count = sum(1 for _ in f)\n",
        "\n",
        "print(f\"Training samples: {train_count}\")\n",
        "print(f\"Validation samples: {val_count}\")\n",
        "print(f\"\\nReady to train {MODEL}!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train model\n",
        "print(f\"Training {MODEL} for {EPOCHS} epochs...\")\n",
        "print(f\"Batch size: {BATCH_SIZE}, Max length: {MAX_LENGTH}\")\n",
        "print(\"This will take ~1-2 hours...\\n\")\n",
        "\n",
        "output_dir = f\"models/zerolang-{MODEL}-colab\"\n",
        "\n",
        "!python training/train_cloud.py \\\n",
        "    --model {MODEL} \\\n",
        "    --data data \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --batch-size {BATCH_SIZE} \\\n",
        "    --max-length {MAX_LENGTH} \\\n",
        "    --output {output_dir}"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Test Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Model mapping\n",
        "BASE_MODELS = {\n",
        "    \"qwen-coder-7b\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    \"qwen-coder-14b\": \"Qwen/Qwen2.5-Coder-14B-Instruct\",\n",
        "    \"qwen-coder-32b\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
        "}\n",
        "\n",
        "model_path = f\"models/zerolang-{MODEL}-colab\"\n",
        "base_model_name = BASE_MODELS[MODEL]\n",
        "\n",
        "print(f\"Loading trained model from {model_path}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_wat(instruction, max_tokens=1024):\n",
        "    \"\"\"Generate WAT code from an instruction.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are ZeroLang, an AI that generates optimized WebAssembly (WAT) code. Output only valid WAT code.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction},\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.2,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with various prompts\n",
        "test_prompts = [\n",
        "    \"Implement: int add(int a, int b)\",\n",
        "    \"Implement: int factorial(int n)\",\n",
        "    \"Implement: int max(int a, int b)\",\n",
        "    \"Implement: int fibonacci(int n)\",\n",
        "    \"Implement: void swap(int *a, int *b)\",\n",
        "]\n",
        "\n",
        "print(\"Testing model outputs...\\n\")\n",
        "for prompt in test_prompts:\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Input: {prompt}\")\n",
        "    print('='*60)\n",
        "    wat = generate_wat(prompt)\n",
        "    # Show first 600 chars\n",
        "    print(wat[:600] if len(wat) > 600 else wat)\n",
        "    print()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Save & Export"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Google Drive\n",
        "if SAVE_TO_DRIVE and DRIVE_OUTPUT:\n",
        "    import shutil\n",
        "    output_name = f\"zerolang-{MODEL}-colab\"\n",
        "    \n",
        "    print(f\"Saving model to Google Drive...\")\n",
        "    \n",
        "    # Copy model\n",
        "    drive_model_path = f\"{DRIVE_OUTPUT}/{output_name}\"\n",
        "    if Path(drive_model_path).exists():\n",
        "        shutil.rmtree(drive_model_path)\n",
        "    shutil.copytree(f\"models/{output_name}\", drive_model_path)\n",
        "    \n",
        "    # Copy training data\n",
        "    shutil.copy(\"data/colab_training.jsonl\", f\"{DRIVE_OUTPUT}/training_data.jsonl\")\n",
        "    \n",
        "    print(f\"‚úÖ Saved to Google Drive: {DRIVE_OUTPUT}\")\n",
        "    !ls -la {DRIVE_OUTPUT}\n",
        "else:\n",
        "    print(\"Google Drive not mounted. Model saved locally only.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download as zip (alternative)\n",
        "import os\n",
        "output_name = f\"zerolang-{MODEL}-colab\"\n",
        "\n",
        "!cd models && zip -r ../{output_name}.zip {output_name}\n",
        "\n",
        "# Include training data in a separate zip\n",
        "!zip -j training_data.zip data/colab_training.jsonl data/train_chatml_colab.jsonl data/val_chatml_colab.jsonl\n",
        "\n",
        "print(f\"\\nCreated zip files:\")\n",
        "!ls -lh *.zip\n",
        "\n",
        "# Uncomment to download\n",
        "# from google.colab import files\n",
        "# files.download(f'{output_name}.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Summary"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üéâ ZeroLang Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load stats\n",
        "with open('data/colab_training.jsonl') as f:\n",
        "    final_pairs = sum(1 for _ in f)\n",
        "\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"  - Data collected: {final_pairs} pairs\")\n",
        "print(f\"  - Model: {MODEL}\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"\\nüìÅ Output:\")\n",
        "print(f\"  - Local: models/zerolang-{MODEL}-colab\")\n",
        "if SAVE_TO_DRIVE and DRIVE_OUTPUT:\n",
        "    print(f\"  - Drive: {DRIVE_OUTPUT}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next steps:\")\n",
        "print(f\"  1. Download the model zip\")\n",
        "print(f\"  2. Use training/inference.py to test locally\")\n",
        "print(f\"  3. Integrate into zrun runtime\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
