{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š Step 1: Data Collection (CPU Only - FREE)\n",
        "\n",
        "**NO GPU NEEDED** - Run this with CPU runtime to save money!\n",
        "\n",
        "This notebook:\n",
        "1. Clones C repositories from GitHub\n",
        "2. Extracts functions and compiles to WAT\n",
        "3. Saves dataset to Google Drive\n",
        "\n",
        "**Time:** ~2-3 hours\n",
        "**Cost:** FREE (CPU only)\n",
        "\n",
        "After this completes, run **Step2_Train_Model.ipynb** with H100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title Configuration { display-mode: \"form\" }\n",
        "\n",
        "TARGET_PAIRS = 10000  #@param {type:\"integer\"}\n",
        "ENABLE_AUGMENTATION = True  #@param {type:\"boolean\"}\n",
        "\n",
        "print(f\"Target: {TARGET_PAIRS} pairs\")\n",
        "print(f\"Augmentation: {'ON' if ENABLE_AUGMENTATION else 'OFF'}\")\n",
        "print(\"\\nâš ï¸ Make sure you're using CPU runtime (Runtime â†’ Change runtime type â†’ None)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ready to collect data\n",
        "print(\"âœ“ Starting data collection pipeline...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/zerolang_data'\n",
        "!mkdir -p {DRIVE_PATH}\n",
        "print(f\"\\nâœ“ Data will be saved to: {DRIVE_PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "# Install dependencies\n",
        "echo \"[1/3] Installing LLVM toolchain...\"\n",
        "apt-get update -qq\n",
        "apt-get install -qq -y llvm lld clang > /dev/null 2>&1\n",
        "\n",
        "echo \"[2/3] Installing wasm-tools...\"\n",
        "cd /tmp\n",
        "curl -sLO https://github.com/bytecodealliance/wasm-tools/releases/download/v1.244.0/wasm-tools-1.244.0-x86_64-linux.tar.gz\n",
        "tar -xzf wasm-tools-1.244.0-x86_64-linux.tar.gz\n",
        "cp wasm-tools-1.244.0-x86_64-linux/wasm-tools /usr/local/bin/\n",
        "chmod +x /usr/local/bin/wasm-tools\n",
        "\n",
        "echo \"[3/3] Verifying installation...\"\n",
        "clang --version | head -1\n",
        "wasm-tools --version\n",
        "\n",
        "# Quick test\n",
        "echo 'int add(int a, int b) { return a + b; }' > /tmp/test.c\n",
        "clang --target=wasm32 -nostdlib -fuse-ld=lld -Wl,--no-entry -Wl,--export-all -o /tmp/test.wasm /tmp/test.c 2>/dev/null\n",
        "if wasm-tools print /tmp/test.wasm > /dev/null 2>&1; then\n",
        "    echo \"âœ“ WASM compilation works!\"\n",
        "else\n",
        "    echo \"âœ— WASM compilation failed\"\n",
        "    exit 1\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Upload project files from local\n",
        "# Download project as ZIP: your local project-zero folder â†’ compress to ZIP\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"ðŸ“¦ Upload project-zero.zip\")\n",
        "print(\"(Compress your local project-zero folder to ZIP and upload)\")\n",
        "print()\n",
        "\n",
        "uploaded = files.upload()\n",
        "zip_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Clean up and extract\n",
        "!rm -rf /content/zerolang\n",
        "with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "    z.extractall('/content/')\n",
        "    \n",
        "# Find and rename extracted folder\n",
        "extracted = [d for d in os.listdir('/content') if 'project-zero' in d.lower() or 'zerolang' in d.lower()]\n",
        "if extracted:\n",
        "    src = f'/content/{extracted[0]}'\n",
        "    if src != '/content/zerolang':\n",
        "        shutil.move(src, '/content/zerolang')\n",
        "        \n",
        "%cd /content/zerolang\n",
        "\n",
        "# Update clang path for Colab\n",
        "!sed -i 's|/opt/homebrew/opt/llvm/bin/clang|clang|g' pipeline/collect_large.py 2>/dev/null || true\n",
        "!sed -i 's|/opt/homebrew/opt/llvm/bin/clang|clang|g' pipeline/generator.py 2>/dev/null || true\n",
        "\n",
        "print(\"âœ“ Project loaded successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# Run large-scale data collection\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "aug_flag = \"\" if ENABLE_AUGMENTATION else \"--no-augment\"\n",
        "\n",
        "print(f\"Starting data collection...\")\n",
        "print(f\"Target: {TARGET_PAIRS} pairs\")\n",
        "print(f\"This will take 2-3 hours...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!python pipeline/collect_large.py \\\n",
        "    --target {TARGET_PAIRS} \\\n",
        "    --output data/large_dataset.jsonl \\\n",
        "    {aug_flag} \\\n",
        "    2>&1 | tee data/collection.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check results\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "data_file = Path('data/large_dataset.jsonl')\n",
        "if data_file.exists():\n",
        "    with open(data_file) as f:\n",
        "        pairs = [json.loads(l) for l in f if l.strip()]\n",
        "    \n",
        "    print(f\"=\"*60)\n",
        "    print(f\"Data Collection Results\")\n",
        "    print(f\"=\"*60)\n",
        "    print(f\"Total pairs: {len(pairs)}\")\n",
        "    \n",
        "    if pairs:\n",
        "        avg_instr = sum(len(p['instruction']) for p in pairs) / len(pairs)\n",
        "        avg_wat = sum(len(p['output']) for p in pairs) / len(pairs)\n",
        "        augmented = sum(1 for p in pairs if p.get('metadata', {}).get('augmented', False))\n",
        "        \n",
        "        print(f\"Original pairs: {len(pairs) - augmented}\")\n",
        "        print(f\"Augmented pairs: {augmented}\")\n",
        "        print(f\"Avg instruction: {avg_instr:.0f} chars\")\n",
        "        print(f\"Avg WAT: {avg_wat:.0f} chars\")\n",
        "else:\n",
        "    print(\"ERROR: No data file found!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split into train/val\n",
        "!python pipeline/postprocess.py split data/large_dataset.jsonl \\\n",
        "    --train data/train_large.jsonl \\\n",
        "    --val data/val_large.jsonl \\\n",
        "    --val-ratio 0.1\n",
        "\n",
        "# Convert to ChatML format\n",
        "!python training/prepare_data.py data/train_large.jsonl -o data/train_chatml_large.jsonl -f chatml\n",
        "!python training/prepare_data.py data/val_large.jsonl -o data/val_chatml_large.jsonl -f chatml\n",
        "\n",
        "print(\"\\nFinal files:\")\n",
        "!wc -l data/*_large*.jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save to Google Drive\n",
        "import shutil\n",
        "\n",
        "files_to_save = [\n",
        "    'data/large_dataset.jsonl',\n",
        "    'data/train_large.jsonl',\n",
        "    'data/val_large.jsonl',\n",
        "    'data/train_chatml_large.jsonl',\n",
        "    'data/val_chatml_large.jsonl',\n",
        "]\n",
        "\n",
        "print(f\"Saving to Google Drive: {DRIVE_PATH}\")\n",
        "for f in files_to_save:\n",
        "    if Path(f).exists():\n",
        "        shutil.copy(f, DRIVE_PATH)\n",
        "        print(f\"  âœ“ {f}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"âœ… Data saved to Google Drive!\")\n",
        "print(f\"=\"*60)\n",
        "print(f\"\\nNext step: Run Step2_Train_Model.ipynb with H100 GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary\n",
        "!ls -lh {DRIVE_PATH}/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}